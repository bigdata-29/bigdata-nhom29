{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe69900",
   "metadata": {},
   "source": [
    "# xử lý dữ lieu job description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15be7ed",
   "metadata": {},
   "source": [
    "Luu y: phai luu file vietnamese-stopwords.txt vao hdfs\n",
    "Có thê lam voi ca tieng anh roi concatenate \n",
    "file stopword tieng anh: stop_words_english.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0368b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tạo list stopwords tiếng việt\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_stopwords_list(file_path):\n",
    "    # Khởi tạo Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"ReadStopwords\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # 1. Đọc file text bằng spark context (trả về một RDD)\n",
    "    # 2. .map(lambda x: x.strip()) để loại bỏ khoảng trắng thừa hoặc ký tự xuống dòng\n",
    "    # 3. .collect() để chuyển dữ liệu từ các worker về máy chủ (driver) dưới dạng list\n",
    "    stopwords_rdd = spark.sparkContext.textFile(file_path)\n",
    "    stopwords_list = stopwords_rdd.map(lambda line: line.strip()).collect()\n",
    "    \n",
    "    # Lọc bỏ các dòng trống nếu có\n",
    "    stopwords_list = [word for word in stopwords_list if word]\n",
    "    \n",
    "    spark.stop()\n",
    "    return stopwords_list\n",
    "\n",
    "# Sử dụng hàm\n",
    "file_path = \"path/to/your/vietnamese_stopwords.txt\"\n",
    "my_stopwords = get_stopwords_list(file_path)\n",
    "\n",
    "print(f\"Số lượng stopwords đã đọc: {len(my_stopwords)}\")\n",
    "print(my_stopwords[:10]) # In thử 10 từ đầu tiên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20061d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# 1. Tách từ (Tokenization)\n",
    "tokenizer = Tokenizer(inputCol=\"description\", outputCol=\"words\")\n",
    "\n",
    "# 2. Loại bỏ từ dừng (Stopwords: \"và\", \"của\", \"the\", \"is\"...)\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "remover.setStopWords(my_stopwords)\n",
    "\n",
    "# 3. Vector hóa (TF-IDF) - Giúp làm nổi bật các từ khóa quan trọng như \"Python\", \"Big Data\"\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# 4. Chuyển nhãn (Label) sang dạng số (ví dụ: Job Category)\n",
    "label_stringIdx = StringIndexer(inputCol=\"category\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24831471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recruitment_analysis.py\n",
    "# ĐÂY LÀ CLASSIFICATION\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def main():\n",
    "    # Khởi tạo Spark Session\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"RecruitmentBigDataAnalysis\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load dữ liệu (CSV/Parquet từ HDFS hoặc S3)\n",
    "    df = spark.read.parquet(\"hdfs:///data/recruitment_data.parquet\")\n",
    "\n",
    "    # Pipeline stages: Feature Engineering + Model\n",
    "    # Giả sử đã định nghĩa các bước tiền xử lý ở trên\n",
    "    lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, label_stringIdx, lr])\n",
    "\n",
    "    # Chia dữ liệu Train/Test\n",
    "    (trainingData, testData) = df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "    # Huấn luyện mô hình\n",
    "    model = pipeline.fit(trainingData)\n",
    "\n",
    "    # Dự đoán\n",
    "    predictions = model.transform(testData)\n",
    "\n",
    "    # Đánh giá độ chính xác\n",
    "    evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Lưu mô hình để sử dụng lại\n",
    "    model.save(\"hdfs:///models/job_classifier_model\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ĐÂY LÀ GỢI Ý RECOMMENDATION MODEL\n",
    "# Dùng ALS (Alternating Least Squares) cho hệ thống gợi ý việc làm dựa trên hồ sơ ứng viên và các công việc đã ứng tuyển trước đó.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682843d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sử dung mô hình sau khi đã train\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# Tải mô hình đã lưu\n",
    "loaded_model = PipelineModel.load(\"hdfs:///models/job_classifier_model\")\n",
    "\n",
    "# Sử dụng mô hình vừa tải để dự đoán\n",
    "new_predictions = loaded_model.transform(fresh_data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
