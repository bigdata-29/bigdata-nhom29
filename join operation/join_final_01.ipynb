{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc69757f",
   "metadata": {},
   "source": [
    "# Tai resume va role dataset tu kaggle ve va luu vao HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: L∆∞u datasetDict IT_role v√†o HDFS d∆∞·ªõi d·∫°ng file raw \n",
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd # D√πng pandas ƒë·ªÉ h·ªó tr·ª£ ki·ªÉm tra v√† thao t√°c (t√πy ch·ªçn)\n",
    "\n",
    "# 1. T·∫£i Dataset\n",
    "IT_role = load_dataset(\"NxtGenIntern/IT_Job_Roles_Skills_Certifications_Dataset\")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ t·∫£i DatasetDict v·ªõi c√°c ph·∫ßn: {list(IT_role.keys())}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. K·∫øt h·ª£p to√†n b·ªô c√°c ph·∫ßn (splits) th√†nh m·ªôt Dataset duy nh·∫•t\n",
    "# print(\"‚û°Ô∏è ƒêang ti·∫øn h√†nh k·∫øt h·ª£p c√°c ph·∫ßn (train, validation, test) th√†nh 1 Dataset...\")\n",
    "\n",
    "# L·∫•y danh s√°ch c√°c Dataset object (v√≠ d·ª•: [IT_role['train'], IT_role['validation']])\n",
    "# all_datasets = list(IT_role.values())\n",
    "\n",
    "# S·ª≠ d·ª•ng h√†m concatenate_datasets ƒë·ªÉ k·∫øt h·ª£p ch√∫ng\n",
    "# combined_dataset = concatenate_datasets(all_datasets)\n",
    "\n",
    "combined_dataset = IT_role['train']\n",
    "\n",
    "print(f\"üéâ ƒê√£ k·∫øt h·ª£p th√†nh c√¥ng. T·ªïng s·ªë h√†ng: {len(combined_dataset)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n v√† t√™n file Parquet\n",
    "OUTPUT_DIR = \"resume_and_role\"\n",
    "OUTPUT_FILENAME = \"IT_role.parquet\"\n",
    "OUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "print(f\"T·∫°o th∆∞ m·ª•c output c·ª•c b·ªô: {OUTPUT_DIR}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 4. L∆∞u Dataset ƒë√£ k·∫øt h·ª£p th√†nh M·ªòT file Parquet\n",
    "# S·ª≠ d·ª•ng ph∆∞∆°ng th·ª©c to_parquet() v·ªõi tham s·ªë writer_batch_size=None \n",
    "# (ho·∫∑c ƒë·∫£m b·∫£o n√≥ kh√¥ng t·∫°o ra nhi·ªÅu file)\n",
    "# Tuy nhi√™n, c√°ch ƒë∆°n gi·∫£n nh·∫•t l√† chuy·ªÉn sang Pandas DataFrame r·ªìi l∆∞u\n",
    "try:\n",
    "    # Chuy·ªÉn ƒë·ªïi Dataset sang Pandas DataFrame\n",
    "    df_combined = combined_dataset.to_pandas()\n",
    "    \n",
    "    # L∆∞u Pandas DataFrame sang M·ªòT file Parquet\n",
    "    df_combined.to_parquet(OUTPUT_PATH, index=False)\n",
    "    \n",
    "    print(f\"üíæ ƒê√£ l∆∞u to√†n b·ªô d·ªØ li·ªáu v√†o M·ªòT file Parquet duy nh·∫•t t·∫°i: {OUTPUT_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"L·ªói khi l∆∞u b·∫±ng Pandas/PyArrow: {e}\")\n",
    "    \n",
    "# TODO: ƒê·∫©y file parquet l√™n HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aee037a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code language\\anaconda3\\envs\\bigdata\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ƒë√£ t·∫£i v·ªÅ: resume_and_role\\master_resumes.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "REPO_ID = \"datasetmaster/resumes\"\n",
    "FILENAME = \"master_resumes.jsonl\"\n",
    "\n",
    "# Ch·ªâ ƒë·ªãnh th∆∞ m·ª•c ƒë√≠ch r√µ r√†ng\n",
    "DEST_DIR = \"./resume_and_role\"\n",
    "\n",
    "file_path = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=FILENAME,\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=DEST_DIR,                # th∆∞ m·ª•c ƒë√≠ch\n",
    "    local_dir_use_symlinks=False       # ƒë·∫£m b·∫£o copy file th·∫≠t, kh√¥ng ch·ªâ symlink\n",
    ")\n",
    "\n",
    "print(\"File ƒë√£ t·∫£i v·ªÅ:\", file_path)\n",
    "\n",
    "os.rename(file_path, os.path.join(DEST_DIR, \"IT_resume.jsonl\"))\n",
    "# IT_resume_df = pd.read_json(\n",
    "#     hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\"),\n",
    "#     lines = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82ee219",
   "metadata": {},
   "source": [
    "# Tai cac du lieu can thiet tu HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecfa9c2",
   "metadata": {},
   "source": [
    "TODO: Thay the cac duong dan den cac file tuong ung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a5928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Kh·ªüi t·∫°o Spark Session (B·∫Øt bu·ªôc)\n",
    "# Th√™m config dfs.client.use.datanode.hostname ƒë·ªÉ fix l·ªói k·∫øt n·ªëi DataNode\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Doc Data Tu HDFS\") \\\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. ƒê∆∞·ªùng d·∫´n HDFS (S·ª≠a l·∫°i host/port cho ƒë√∫ng v·ªõi setup c·ªßa b·∫°n)\n",
    "# N·∫øu ch·∫°y t·ª´ local m√°y t√≠nh k·∫øt n·ªëi v√†o Docker: d√πng localhost ho·∫∑c host.docker.internal\n",
    "\n",
    "# Ho·∫∑c n·∫øu ch·∫°y production/internal network:\n",
    "# hdfs_path = \"hdfs://namenode:9000/path/to/file.jsonl\"\n",
    "\n",
    "# c√°c b·∫£ng job\n",
    "itviec_path = \"hdfs://host.docker.internal:9000/path/to/file.jsonl\" \n",
    "careerviet_path = \"hdfs://host.docker.internal:9000/path/to/file.jsonl\" \n",
    "vietnamwork_path = \"hdfs://host.docker.internal:9000/path/to/file.jsonl\" \n",
    "topcv_path = \"hdfs://host.docker.internal:9000/path/to/file.jsonl\" \n",
    "\n",
    "# b·∫£ng resume\n",
    "resume_path = \"hdfs://host.docker.internal:9000/path/to/IT_resume.jsonl\"\n",
    "\n",
    "\n",
    "# b·∫£ng skill\n",
    "role_path = \"hdfs://host.docker.internal:9000/path/to/IT_role.parquet\"\n",
    "\n",
    "\n",
    "# 3. ƒê·ªçc file\n",
    "# Do Spark ho·∫°t ƒë·ªông theo c∆° ch·∫ø Lazy Evaluation, d√≤ng n√†y ch·ªâ m·ªõi check metadata\n",
    "df_itviec = spark.read.json(itviec_path)\n",
    "df_CareerViet = spark.read.json(careerviet_path)\n",
    "df_VietnamWork = spark.read.json(vietnamwork_path)\n",
    "df_TopCV = spark.read.json(topcv_path)\n",
    "\n",
    "df_resume = spark.read.json(resume_path)\n",
    "df_role = spark.read.parquet(role_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23248c",
   "metadata": {},
   "source": [
    "## Tach bang company tu df_itviec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e7b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_company_detail = df_itviec.select(\n",
    "    \n",
    "    \n",
    "    # Tr√≠ch xu·∫•t c√°c tr∆∞·ªùng con t·ª´ 'company_detail'\n",
    "    col(\"company_detail.company_name\").alias(\"main_company_name\"),\n",
    "    col(\"company_detail.declaration\"),\n",
    "    col(\"company_detail.`Company type`\").alias(\"company_type\"), # D√πng d·∫•u `` cho t√™n c·ªôt c√≥ kho·∫£ng tr·∫Øng\n",
    "    col(\"company_detail.`Company industry`\").alias(\"company_industry\"),\n",
    "    col(\"company_detail.`Company size`\").alias(\"company_size\"),\n",
    "    col(\"company_detail.Country\"),\n",
    "    col(\"company_detail.`Working days`\").alias(\"working_days\"),\n",
    "    col(\"company_detail.`Overtime policy`\").alias(\"overtime_policy\")\n",
    "    \n",
    ").distinct()\n",
    "\n",
    "# X√≥a tr∆∞·ªùng company kh·ªèi df_itviec ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi join\n",
    "df_itviec = df_itviec.drop(\"company_detail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d02ec4",
   "metadata": {},
   "source": [
    "# G·ªôp c√°c b·∫£ng job v·ªõi nhau th√†nh b·∫£ng l·ªõn theo union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6490675d",
   "metadata": {},
   "source": [
    "S·ª≠ d·ª•ng union chay ƒë·ªÉ g·ªçp m√† kh√¥ng c·∫ßn bi·∫øt t√™n c·ªôt nh∆∞ th·∫ø n√†o\n",
    "ƒêi·ªÅu ti√™n quy·∫øt ph·∫£i l√† th·ª© t·ª± c√°c c·ªôt ph·∫£i gi·ªëng h·ªát nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ff77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S·ª≠ d·ª•ng union chay ƒë·ªÉ g·ªçp m√† kh√¥ng c·∫ßn bi·∫øt t√™n c·ªôt nh∆∞ th·∫ø n√†o\n",
    "# ƒêi·ªÅu ti√™n quy·∫øt ph·∫£i l√† th·ª© t·ª± c√°c c·ªôt ph·∫£i gi·ªëng h·ªát nhau\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "list_df_jobs = [df_itviec, df_CareerViet, df_VietnamWork, df_TopCV]\n",
    "\n",
    "df_job = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), list_df_jobs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1579edeb",
   "metadata": {},
   "source": [
    "# Join broadcast v·ªõi b·∫£ng company v√† job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6dde8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr∆∞·ªõc ƒë√≥ c√≥ th·ªÉ c·∫•u h√¨nh broadcast c·ª±c nh·ªè ƒë·ªÉ ƒë√°m b·∫£o ch·ªâ 1 b·∫£ng ƒë∆∞·ª£c broadcast\n",
    "# spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1024 )  # 1KB\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_job_company = df_job.join(\n",
    "    # S·ª≠ d·ª•ng h√†m broadcast() tr√™n DataFrame nh·ªè h∆°n\n",
    "    broadcast(df_company_detail), \n",
    "    df_job[\"company_name\"] == df_company_detail[\"main_company_name\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# TODO: L∆∞u df_job_company n√†y l√™n cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fbba1",
   "metadata": {},
   "source": [
    "# TODO:L∆∞u df_job_company n√†y l√™n cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c03296",
   "metadata": {},
   "source": [
    "# Join b·∫£ng df_job_company v·ªõi df_role theo key v·ªã tr√≠ c√¥ng vi·ªác (role - job title)\n",
    "TODO: c√≥ th·ªÉ s·ª≠ d·ª•ng ch√≠nh b·∫£ng df_job_company ƒë√£ ƒë∆∞·ª£c cache ·ªü tr√™n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff4646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join b·∫£ng df_job_company v·ªõi df_role theo key v·ªã tr√≠ c√¥ng vi·ªác (role - job title)\n",
    "\n",
    "# b·∫Øt bu·ªôc config ƒë·ªÉ ƒë·∫£m b·∫£o sort merge join thay v√¨ broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 1024 )  # 1KB\n",
    "\n",
    "df_job_company_role = df_job_company.join(\n",
    "    df_role,\n",
    "    df_job_company['Role']== df_role['Job Title'],\n",
    "    'inner'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba620644",
   "metadata": {},
   "source": [
    "# Multiple joins v·ªõi df_resume theo key t√™n ·ª©ng vi√™n (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b70fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple joins v·ªõi df_resume theo key t√™n ·ª©ng vi√™n (name)\n",
    "\n",
    "\n",
    "# N√™n s·ª≠ d·ª•ng CBO cho multiple join\n",
    "\n",
    "# ƒê·ªÉ s·ª≠ d·ª•ng CBO th√¨ ch·ªâ c·∫ßn config trong Spark Session v√† th·ªëng k√™ c√°c b·∫£ng tr∆∞·ªõc (v√¨ CBO ch·ªâ l√†m vi·ªác v·ªõi c√°c b·∫£ng ƒë√£ c√≥ th·ªëng k√™)\n",
    "\n",
    "\n",
    "# 1. KH·ªûI T·∫†O SPARK SESSION V√Ä B·∫¨T CBO\n",
    "# C·∫•u h√¨nh CBO v√† c√°c tham s·ªë li√™n quan\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.statistics.fallBackToHadoopFileLength\", \"false\")\n",
    "\n",
    "# ƒê·∫£m b·∫£o CBO ƒë∆∞·ª£c b·∫≠t\n",
    "print(f\"CBO Enabled: {spark.conf.get('spark.sql.cbo.enabled')}\")\n",
    "\n",
    "# --- D·ªÆ LI·ªÜU GI·∫¢ L·∫¨P ---\n",
    "\n",
    "\n",
    "# 2. ƒêƒÇNG K√ù B·∫¢NG T·∫†M V√Ä THU TH·∫¨P TH·ªêNG K√ä (ANALYZE)\n",
    "# CBO ch·ªâ ho·∫°t ƒë·ªông tr√™n c√°c b·∫£ng ƒë√£ c√≥ th·ªëng k√™.\n",
    "df_job.createOrReplaceTempView(\"job\")\n",
    "df_role.createOrReplaceTempView(\"role\")\n",
    "df_company_detail.createOrReplaceTempView(\"company\")\n",
    "\n",
    "print(\"\\n--- B·∫Øt ƒë·∫ßu thu th·∫≠p Th·ªëng k√™ ---\")\n",
    "# Thu th·∫≠p th·ªëng k√™ cho t·∫•t c·∫£ c√°c b·∫£ng v√† c·ªôt\n",
    "spark.sql(\"ANALYZE TABLE job COMPUTE STATISTICS FOR ALL COLUMNS\").show()\n",
    "spark.sql(\"ANALYZE TABLE role COMPUTE STATISTICS FOR ALL COLUMNS\").show()\n",
    "spark.sql(\"ANALYZE TABLE company COMPUTE STATISTICS FOR ALL COLUMNS\").show()\n",
    "print(\"--- K·∫øt th√∫c thu th·∫≠p Th·ªëng k√™ ---\")\n",
    "\n",
    "# 3. TH·ª∞C HI·ªÜN TRUY V·∫§N V·ªöI NHI·ªÄU PH√âP JOIN (3 B·∫¢NG)\n",
    "# Y√™u c·∫ßu Spark t·ª± ƒë·ªông x√°c ƒë·ªãnh th·ª© t·ª± join t·ªëi ∆∞u:\n",
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    t1.ten, \n",
    "    t2.ten_ky_nang, \n",
    "    t3.cap_bac\n",
    "FROM \n",
    "    job t1\n",
    "INNER JOIN \n",
    "    role t2\n",
    "ON \n",
    "    t1.role = t2['Job Title']\n",
    "INNER JOIN\n",
    "    company t3\n",
    "ON \n",
    "    t1.company_name = t3.main_company_name\n",
    "\"\"\"\n",
    "\n",
    "df_job_company_role = spark.sql(sql_query)\n",
    "\n",
    "# 4. KI·ªÇM TRA K·∫æ HO·∫†CH TH·ª∞C THI\n",
    "print(\"\\n--- K·∫æ HO·∫†CH TH·ª∞C THI (Execution Plan) ƒë∆∞·ª£c t·ªëi ∆∞u b·ªüi CBO ---\")\n",
    "# CBO s·∫Ω quy·∫øt ƒë·ªãnh th·ª© t·ª± join v√† lo·∫°i join (BroadcastHashJoin, SortMergeJoin)\n",
    "# d·ª±a tr√™n k√≠ch th∆∞·ªõc v√† th·ªëng k√™ ƒë√£ thu th·∫≠p ƒë∆∞·ª£c.\n",
    "df_job_company_role.explain(extended=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd6ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41816f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
