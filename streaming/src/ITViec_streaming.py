from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

from webdriver_manager.chrome import ChromeDriverManager 
from kafka_producer import JobProducer

import datetime
import time
import os
import random
from bs4 import BeautifulSoup
import re
SCREEN_WIDTH = 1920
SCREEN_HEIGHT = 1080
BATCH_SIZE = 25

def setup_driver():
    chrome_options = Options()
    # ThÃªm Ä‘Æ°á»ng dáº«n tá»›i Chrome browser
    # chrome_options.binary_location = "C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe"  # ÄÆ°á»ng dáº«n máº·c Ä‘á»‹nh cá»§a Chrome
    
    chrome_options.add_argument("--app")
    chrome_options.add_argument(f"--window-size={SCREEN_WIDTH},{SCREEN_HEIGHT}")
    chrome_options.add_argument("--window-position=0,0")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")
    prefs = {"profile.managed_default_content_settings.images": 2}
    chrome_options.add_experimental_option("prefs", prefs)
    #chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver


def sign_in(driver, email, password):
    driver.get("https://itviec.com/sign_in")

    time.sleep(2)

    email_input = driver.find_element(By.NAME, "user[email]")
    password_input = driver.find_element(By.NAME, "user[password]")

    email_input.send_keys(email)
    password_input.send_keys(password)

    submit_button = driver.find_element(By.XPATH, "//button[@type='submit']")
    submit_button.click()

    time.sleep(2)
    print("ÄÄƒng nháº­p thÃ nh cÃ´ng!")
    
def crawl_links(driver):
    driver.get("https://itviec.com/it-jobs")
    time.sleep(random.uniform(2, 3))
    links = []
    while True:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        job_cards = soup.find_all('div', class_='job-card')
        for job_card in job_cards:
            # TÃ¬m kiáº¿m táº¥t cáº£ cÃ¡c link cÃ´ng viá»‡c trong chÃ­nh tháº» Ä‘Ã³
            job_url_attribute = 'data-search--job-selection-job-url-value'
            
            if job_url_attribute in job_card.attrs:
                relative_link = job_card.attrs[job_url_attribute]
                relative_link = relative_link.split('/content')[0] # loáº¡i bá» pháº§n dÆ° thá»«a Ä‘á»ƒ truy cáº­p vÃ o Ä‘Ãºng link 
                full_link = "https://itviec.com" + relative_link
                links.append(full_link)
            else:
                print("ERROR: KhÃ´ng tÃ¬m tháº¥y thuá»™c tÃ­nh link trong tháº» job-card.")
                
        print("------------------------------------")
        print('Ä‘ang á»Ÿ trang sá»‘:', len(links)//BATCH_SIZE)
        print("ÄÃ£ thu tháº­p Ä‘Æ°á»£c", len(links), "liÃªn káº¿t cÃ´ng viá»‡c.")
        try:
            next_page = soup.find('a', rel='next')
            if next_page and next_page['href']:
                driver.get("https://itviec.com" + next_page['href'])
                time.sleep(random.uniform(2, 3))
            else:
                print("KhÃ´ng tÃ¬m tháº¥y trang tiáº¿p theo. Káº¿t thÃºc thu tháº­p liÃªn káº¿t.")
                break
            
        except Exception as e:
            print("ÄÃ£ háº¿t trang hoáº·c cÃ³ lá»—i xáº£y ra:", e)
            break
        
    return links



    
def parse_detail_job(driver, link):
    driver.get(link)
    time.sleep(random.uniform(3, 5))
    content = driver.page_source
    
    soup = BeautifulSoup(content, 'html.parser')
    job_title = soup.find('div', class_='job-header-info').find('h1').get_text(strip=True)
    company_name = soup.find('div', class_='employer-name').get_text(strip=True)
    salary = soup.find('div', class_='salary').get_text(strip=True)
    
    
    
    job_show_info = soup.find('div', class_='job-show-info').find_all('span')
    job_show_info_list = [info.get_text().replace('\n',' ') for info in job_show_info]
    company_location  = job_show_info_list[0]
    woring_model = job_show_info_list[1]
    post_time = job_show_info_list[2]
    #skill and job_expertise
    skills, job_expertise = soup.find('div', class_='job-show-info').find_all('div', class_='imb-4 imb-xl-3 d-flex flex-column flex-xl-row igap-3 align-items-xl-baseline')
    skills_list = [skill.get_text(strip=True) for skill in skills.find_all('a')]
    job_expertise_list = [exp.get_text(strip=True) for exp in job_expertise.find_all('a')]
    
    #job_domain
    job_domain = soup.find_all('div', class_='itag bg-light-grey itag-sm cursor-default')
    job_domain = [domain.get_text(strip=True) for domain in job_domain]
    
    #job description, your_skill_and_experience, benefit
    job_description, your_skill_and_experience, benefit = soup.find_all('div', class_='imy-5 paragraph')
    job_description = job_description.get_text(strip=True, separator='\n')
    your_skill_and_experience = your_skill_and_experience.get_text(strip=True, separator='\n')
    benefit = benefit.get_text(strip=True, separator='\n')
    
    # company detail vá»›i pattern Ä‘á»ƒ chá»‰ chá»n Ä‘Ãºng class
    company_detail = {}
    pattern_1 = re.compile(r'\b' + re.escape('job-show-employer-info') + r'\b')
    job_show_employer_info = soup.find('section', class_=pattern_1)
    
    if job_show_employer_info:
        company_detail['company_name'] = job_show_employer_info.find('h3').get_text(strip=True)
        company_detail['declaration'] = job_show_employer_info.find('div', class_='imt-5 imt-xl-4').get_text(strip=True)
        
        pattern_2 = re.compile(r'\b' + re.escape('row ipy-2') + r'\b')
        info_items = job_show_employer_info.find_all('div', class_=pattern_2)
        for item in info_items:
            item = item.get_text(strip=True, separator='\n')
            key = item.split('\n')[0]
            value = item.split('\n')[1]
            company_detail[key] = value
    
    return {
        'job_title': job_title,
        'company_name': company_name,
        'salary': salary,
        'company_location': company_location,
        'woring_model': woring_model,
        'post_time': post_time,
        'skills_list': skills_list,
        'job_expertise': job_expertise_list,
        'job_domain': job_domain,
        'job_description': job_description,
        'your_skill_and_experience': your_skill_and_experience,
        'benefit': benefit,
        'company_detail': company_detail,
        
    }
    
def run_crawl(email='dihidromonooxit01012000@gmail.com', password='1!Aaaaaaaaaa'):
    """
    Cháº¡y crawler, cÃ o dá»¯ liá»‡u vÃ  gá»­i tá»«ng job vÃ o Kafka ngay láº­p tá»©c.
    """
    # 1. Khá»Ÿi táº¡o Producer á»Ÿ Ä‘áº§u
    producer = JobProducer()
    if not producer.producer: # Náº¿u khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c Kafka thÃ¬ dá»«ng luÃ´n
        return

    driver = setup_driver()
    job_sent_count = 0

    # 2. DÃ¹ng try...finally Ä‘á»ƒ Ä‘áº£m báº£o producer luÃ´n Ä‘Æ°á»£c Ä‘Ã³ng
    try:
        try:
            sign_in(driver, email, password)
        except Exception as e:
            print(f'Warning: khÃ´ng thá»ƒ Ä‘Äƒng nháº­p hoáº·c bá» qua Ä‘Äƒng nháº­p: {e}')

        links = crawl_links(driver)
        print(f'ÄÃ£ thu tháº­p Ä‘Æ°á»£c {len(links)} links. Báº¯t Ä‘áº§u cÃ o chi tiáº¿t...')

        for i, link in enumerate(links):
            try:
                # CÃ o chi tiáº¿t má»™t job
                detail = parse_detail_job(driver, link)

                # 3. THÃŠM CÃC TRÆ¯á»œNG Dá»® LIá»†U QUAN TRá»ŒNG
                # ThÃªm nguá»“n dá»¯ liá»‡u Ä‘á»ƒ Spark biáº¿t tin nÃ y tá»« Ä‘Ã¢u
                detail['source'] = 'itviec.com'
                # ThÃªm timestamp thá»i Ä‘iá»ƒm cÃ o dá»¯ liá»‡u
                detail['crawled_at'] = datetime.datetime.now().isoformat()
                detail['url'] = link # ThÃªm cáº£ URL cá»§a tin tuyá»ƒn dá»¥ng

                # 4. Gá»¬I Dá»® LIá»†U VÃ€O KAFKA
                producer.send_job(detail)
                job_sent_count += 1
                print(f"[{i+1}/{len(links)}] ÄÃ£ gá»­i job '{detail['job_title']}' tá»« {detail['company_name']} vÃ o Kafka.")

            except Exception as e:
                print(f'Lá»—i khi parse {link} (link thá»© {i+1}): {e}. Bá» qua.')
                continue

    finally:
        # 5. LuÃ´n Ä‘Ã³ng driver vÃ  producer khi káº¿t thÃºc
        print("\nHoÃ n táº¥t quÃ¡ trÃ¬nh cÃ o dá»¯ liá»‡u.")
        print(f"Tá»•ng sá»‘ tin tuyá»ƒn dá»¥ng Ä‘Ã£ gá»­i vÃ o Kafka: {job_sent_count}")
        if driver:
            driver.quit()
        producer.close()

# def crawl_first_page_links(driver):
#     """Má»™t phiÃªn báº£n sá»­a Ä‘á»•i cá»§a crawl_links, chá»‰ láº¥y link á»Ÿ trang Ä‘áº§u tiÃªn."""
#     driver.get("https://itviec.com/it-jobs")
#     #time.sleep(random.uniform(2, 3))
#     time.sleep(random.uniform(8, 10))
#     links = []
#     soup = BeautifulSoup(driver.page_source, 'html.parser')
#     #job_cards = soup.find_all('div', class_='job-card')
#     job_cards = soup.select('div.job-card')

#     if not job_cards:
#         print("Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y job card nÃ o báº±ng CSS selector 'div.job-card'. CÃ³ thá»ƒ cáº¥u trÃºc trang Ä‘Ã£ thay Ä‘á»•i hoÃ n toÃ n.")
    
#     for job_card in job_cards:
#         job_url_attribute = 'data-search--job-selection-job-url-value'
#         if job_url_attribute in job_card.attrs:
#             relative_link = job_card.attrs[job_url_attribute].split('/content')[0]
#             full_link = "https://itviec.com" + relative_link
#             links.append(full_link)
#     print(f"ÄÃ£ thu tháº­p Ä‘Æ°á»£c {len(links)} links tá»« trang Ä‘áº§u tiÃªn.")
#     return links

def crawl_first_page_links(driver):
    """
    Má»™t phiÃªn báº£n nÃ¢ng cáº¥p, sá»­ dá»¥ng WebDriverWait Ä‘á»ƒ Ä‘áº£m báº£o cÃ¡c job card Ä‘Ã£ Ä‘Æ°á»£c táº£i xong.
    """
    print("Äang truy cáº­p trang viá»‡c lÃ m vÃ  chá» dá»¯ liá»‡u Ä‘á»™ng...")
    driver.get("https://itviec.com/it-jobs")

    try:
        # ===== THAY Äá»”I QUAN TRá»ŒNG NHáº¤T =====
        # Chá» tá»‘i Ä‘a 15 giÃ¢y cho Ä‘áº¿n khi ÃT NHáº¤T Má»˜T tháº» div cÃ³ class 'job-card' xuáº¥t hiá»‡n.
        # Ngay khi nÃ³ xuáº¥t hiá»‡n, code sáº½ tiáº¿p tá»¥c cháº¡y mÃ  khÃ´ng cáº§n chá» háº¿t 15 giÃ¢y.
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.job-card"))
        )
        print("-> CÃ¡c job card Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng!")

    except Exception as e:
        # Náº¿u sau 15 giÃ¢y mÃ  váº«n khÃ´ng tháº¥y, in ra lá»—i vÃ  tráº£ vá» danh sÃ¡ch rá»—ng.
        print(f"!!! Lá»—i: KhÃ´ng tÃ¬m tháº¥y job card nÃ o sau 15 giÃ¢y. CÃ³ thá»ƒ trang Ä‘Ã£ thay Ä‘á»•i hoáº·c cÃ³ lá»—i táº£i trang. Lá»—i: {e}")
        # LÆ°u láº¡i HTML Ä‘á»ƒ debug xem táº¡i sao
        with open("debug_itviec_timeout.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        print("!!! ÄÃ£ lÆ°u HTML lÃºc bá»‹ lá»—i vÃ o file 'debug_itviec_timeout.html'")
        return []

    # BÃ¢y giá», khi chÃºng ta cháº¯c cháº¯n cÃ¡c job card Ä‘Ã£ á»Ÿ Ä‘Ã³, chÃºng ta má»›i láº¥y page_source
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    job_cards = soup.select('div.job-card')
    links = []
    for job_card in job_cards:
        job_url_attribute = 'data-search--job-selection-job-url-value'
        if job_url_attribute in job_card.attrs:
            relative_link = job_card.attrs[job_url_attribute].split('/content')[0]
            full_link = "https://itviec.com" + relative_link
            links.append(full_link)
    
    print(f"ÄÃ£ thu tháº­p Ä‘Æ°á»£c {len(links)} links tá»« trang Ä‘áº§u tiÃªn.")
    return links

#def run_continuous_crawl(interval_seconds=15, email='dihidromonooxit01012000@gmail.com', password='1!Aaaaaaaaaa'): # 1800 giÃ¢y = 30 phÃºt
    """
    Cháº¡y crawler á»Ÿ cháº¿ Ä‘á»™ streaming liÃªn tá»¥c, Ä‘á»‹nh ká»³ cÃ o trang Ä‘áº§u tiÃªn.
    """
    producer = JobProducer()
    if not producer.producer: return

    driver = None
    total_jobs_sent = 0

    try:
        driver = setup_driver()
        sign_in(driver, email, password)
        
        while True: # VÃ²ng láº·p vÃ´ háº¡n Ä‘á»ƒ cháº¡y liÃªn tá»¥c
            print(f"\n{'='*50}")
            print(f"Báº¯t Ä‘áº§u chu ká»³ cÃ o dá»¯ liá»‡u má»›i lÃºc: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
            links = crawl_first_page_links(driver)
            session_job_count = 0
            for i, link in enumerate(links):
                try:
                    # TODO: ThÃªm logic Ä‘á»ƒ kiá»ƒm tra xem link nÃ y Ä‘Ã£ Ä‘Æ°á»£c cÃ o trÆ°á»›c Ä‘Ã³ chÆ°a (dÃ¹ng DB/cache)
                    # Náº¿u Ä‘Ã£ cÃ o rá»“i thÃ¬ `continue`
                    
                    detail = parse_detail_job(driver, link)
                    detail['source'] = 'itviec.com'
                    # ThÃªm timestamp thá»i Ä‘iá»ƒm cÃ o dá»¯ liá»‡u
                    detail['crawled_at'] = datetime.datetime.now().isoformat()
                    producer.send_job(detail)
                    session_job_count += 1
                except Exception as e:
                    print(f"Lá»—i khi xá»­ lÃ½ link {link}: {e}")
            
            total_jobs_sent += session_job_count
            print(f"Chu ká»³ hoÃ n táº¥t. ÄÃ£ gá»­i {session_job_count} tin má»›i. Tá»•ng cá»™ng Ä‘Ã£ gá»­i: {total_jobs_sent}")
            print(f"Sáº½ nghá»‰ trong {interval_seconds / 60:.1f} phÃºt...")
            print(f"{'='*50}\n")
            time.sleep(interval_seconds)

    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ÄÃ£ nháº­n tÃ­n hiá»‡u ngáº¯t (Ctrl+C). Äang dá»«ng streaming...")
    finally:
        print("\nÄang dá»n dáº¹p vÃ  thoÃ¡t...")
        if driver: driver.quit()
        if producer: producer.close()
def run_continuous_crawl(interval_seconds=60, email='dihidromonooxit01012000@gmail.com', password='1!Aaaaaaaaaa'):
    """
    Cháº¡y crawler á»Ÿ cháº¿ Ä‘á»™ streaming liÃªn tá»¥c, tÃ¡i táº¡o káº¿t ná»‘i Kafka má»—i chu ká»³.
    """
    driver = None
    total_jobs_sent = 0

    try:
        driver = setup_driver()
        sign_in(driver, email, password)

        while True:
            print(f"\n{'='*50}")
            print(f"Báº¯t Ä‘áº§u chu ká»³ cÃ o dá»¯ liá»‡u má»›i lÃºc: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            # 1. Táº O PRODUCER Má»šI á» Äáº¦U Má»–I CHU Ká»²
            producer = JobProducer()
            if not producer.producer:
                print("KhÃ´ng thá»ƒ táº¡o producer, sáº½ thá»­ láº¡i á»Ÿ chu ká»³ sau.")
                time.sleep(interval_seconds)
                continue # Bá» qua chu ká»³ nÃ y vÃ  thá»­ láº¡i

            session_job_count = 0
            try:
                links = crawl_first_page_links(driver)
                for i, link in enumerate(links):
                    try:
                        #todo: ThÃªm logic Ä‘á»ƒ kiá»ƒm tra xem link nÃ y Ä‘Ã£ Ä‘Æ°á»£c cÃ o trÆ°á»›c Ä‘Ã³ chÆ°a (dÃ¹ng DB/cache)
                        # Náº¿u Ä‘Ã£ cÃ o rá»“i thÃ¬ `continue`
                        detail = parse_detail_job(driver, link)
                        detail['source'] = 'itviec.com'
                        detail['crawled_at'] = datetime.datetime.now().isoformat()
                        detail['url'] = link
                        
                        producer.send_job(detail)
                        session_job_count += 1
                    except Exception as e:
                        print(f"Lá»—i khi xá»­ lÃ½ link {link}: {e}")
            
            finally:
                # 2. ÄÃ“NG PRODUCER á» CUá»I Má»–I CHU Ká»²
                print(f"Chu ká»³ hoÃ n táº¥t. ÄÃ£ gá»­i {session_job_count} tin má»›i.")
                if producer:
                    producer.close()

            total_jobs_sent += session_job_count
            print(f"Tá»•ng cá»™ng Ä‘Ã£ gá»­i: {total_jobs_sent}")
            print(f"Sáº½ nghá»‰ trong {interval_seconds / 60:.1f} phÃºt...")
            print(f"{'='*50}\n")
            time.sleep(interval_seconds)

    except KeyboardInterrupt:
        print("\n\nÄÃ£ nháº­n tÃ­n hiá»‡u ngáº¯t (Ctrl+C). Äang dá»«ng streaming...")
    finally:
        print("\nÄang dá»n dáº¹p vÃ  thoÃ¡t...")
        if driver:
            driver.quit()
        # KhÃ´ng cáº§n Ä‘Ã³ng producer á»Ÿ Ä‘Ã¢y ná»¯a vÃ¬ nÃ³ Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã³ng trong vÃ²ng láº·p
if __name__ == '__main__':
    #run_crawl()
    run_continuous_crawl()